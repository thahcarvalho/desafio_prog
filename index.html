<!DOCTYPE html>
<html>
<head>
  <title>IA</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="css/style.css">
  <link href="https://fonts.googleapis.com/css?family=Slabo+27px" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <style>
  body {
      position: relative;
  }
  #section1 {padding-left:0px;padding-right:0px;}
  #section2 {padding-top:35px;}
  #section3 {padding-top:35px;}
  #section4 {padding-top:35px;}
  #section5 {padding-top:35px;}
  #section6 {padding-top:35px;}
  #section7 {padding-top:35px; padding-bottom: 35px;}
  #section8 {padding-top:35px;padding-bottom:50px; background-color: #222222; color: #fff;}
  </style>
</head>
<body data-spy="scroll" data-target=".navbar" data-offset="50">

<nav class="navbar navbar-inverse navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Artificial Intelligence</a>
    </div>
    <div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav">
          <li><a href="#section2">Introduction</a></li>
          <li><a href="#section3">Why work on this problem?</a></li>
          <li><a href="#section4">What are the major arguments...</a></li>
          <li><a href="#section5">What can you do to help?</a></li>
          <li><a href="#section6">List of all our articles on AI</a></li>
          <li><a href="#section7">Learn more</a></li>
          <li><a href="#section8">Send us a suggestion</a></li>

        </ul>
      </div>
    </div>
  </div>
</nav>

<div id="section1" class="container-fluid">
  <img src="img/ia.jpg" width="100%">
</div>
<div id="section2" class="container-fluid">
  <h1>Introduction</h1>
  <p>Many experts believe that there is a significant chance that humanity will develop machines more intelligent than ourselves during the 21st century. This could lead to large, rapid improvements in human welfare, but there are good reasons to think that it could also lead to disastrous outcomes. The problem of how one might design a highly intelligent machine to pursue realistic human goals safely is very poorly understood. If AI research continues to advance without enough work going into the research problem of controlling such machines, catastrophic accidents are much more likely to occur. Despite growing recognition of this challenge, fewer than 100 people worldwide are directly working on the problem.</p>
</div>
<div id="section3" class="container-fluid">
  <h1>Why work on this problem?</h1>
  <p>The arguments for working on this problem area are complex, and what follows is only a brief summary. If you prefer video, then see this TED talk:</p>
  <center><iframe width="560" height="315" src="https://www.youtube.com/embed/MnT1xgZgkpk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></center>

  <h3>Recent progress in machine learning suggests that AI’s impact may be large and sudden</h3>
  <p>When Tim Urban started investigating <a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html" target="_blank">his article</a> on this topic, he expected to finish it in a few days. Instead he spent weeks reading everything he could, because, he says, <i>“it hit me pretty quickly that what’s happening in the world of AI is not just an important topic, but by far the most important topic for our future.”</i></p>

  <p>In October 2015 an AI system named AlphaGo shocked the world by defeating a professional at the ancient Chinese board game of Go for the first time. A mere five months later, a second shock followed: AlphaGo had bested one of the world’s top Go professionals, winning 4 matches out of 5. Seven months later, the same program had further improved, crushing the world’s top players in a 60-win streak. In the span of a year, AI had advanced from being too weak to win a single match against the worst human professionals, to being impossible for even the best players in the world to defeat.</p>
  <p>This was shocking because Go is considered far harder for a machine to play than Chess. The number of possible moves in Go is vast, so it’s not possible to work out the best move through “brute force”. Rather, the game requires strategic intuition. Some experts thought it would take at least a decade for Go to be conquered.</p>
  <p>Since then, AlphaGo has discovered that certain ways of playing Go that humans had dismissed as foolish for thousands of years were actually superior. Ke Jie, the top ranked go player in the world, has been astonished: <i>“after humanity spent thousands of years improving our tactics,”</i> he said, <i>“computers tell us that humans are completely wrong. I would go as far as to say not a single human has touched the edge of the truth of Go.”</i></p>
  <p>The advances above became possible due to progress in an AI technique called “deep learning”. In the past, we had to give computers detailed instructions for every task. Today, we have programs that teach themselves how to achieve a goal – for example, a program was able to <a href="https://deepmind.com/blog/deep-reinforcement-learning/" target="_blank">learn how to play Atari games</a> based only on reward feedback from the score. This has been made possible by improved algorithms, faster processors, bigger data sets, and huge investments by companies like Google. It has led to <a href="https://medium.com/transmission-newsletter/deep-learning-is-revolutionary-d0f3667bafa0" target="_blank">amazing advances</a> far faster than expected.</i>
  <p>But those are just games. Is general machine intelligence still far away? Maybe, but maybe not. It is really hard to predict the future of technology, and lots of past attempts have been completely off the mark. However, the best available surveys of experts assign a significant probability to the development of powerful AI within our lifetimes.</p>
  <p>One survey of the 100 most-cited living computer science researchers, of whom 29 responded, found that <a href="http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines" target="_blank">more than half</a> thought there was a greater than 50% chance of “high-level machine intelligence” – one that can carry out most human professions at least as well as a typical human – being created by 2050, and a greater than 10% chance of it happening by 2024 (see figure below).</p>

  <center><img src="img/section2-1.png"></center>

  <h3>When superintelligent AI arrives, it could have huge positive and negative impacts</h3>
  <p>If the experts are right, an AI system that reaches and then exceeds human capabilities could have very large impacts, both positive and negative. If AI matures in fields such as mathematical or scientific research, these systems could make rapid progress in curing diseases or engineering robots to serve human needs.</p>

  <p>On the other hand, many people worry about the disruptive social effects of this kind of machine intelligence, and in particular its capacity to take over jobs previously done by less skilled workers. If the economy is unable to create new jobs for these people quickly enough, there will be widespread unemployment and falling wages.13 These outcomes could be avoided through government policy, but doing so would likely require significant planning.</p>

  <p>However, those aren’t the only impacts highly intelligent machines could have.</p>

  <p>Professor Stuart Russell, who wrote the leading textbook on artificial intelligence, has written:</p>




  <blockquote cite="">
<i>Success brings huge risks. … the combination of [goal] misalignment with increasingly capable decision-making systems can lead to problems – perhaps even species-ending problems if the machines are more capable than humans.</i>
</blockquote>

<p>Here is a highly simplified example of the concern:</p>
  <blockquote cite="">
    <p>The owners of a pharmaceutical company use machine learning algorithms to rapidly generate and evaluate new organic compounds.</p>
    <p>As the algorithms improve in capability, it becomes increasingly impractical to keep humans involved in the algorithms’ work – and the humans’ ideas are usually worse anyway. As a result, the system is granted more and more autonomy in designing and running experiments on new compounds.</p>
    <p>Eventually the algorithms are assigned the goal of “reducing the incidence of cancer,” and offer up a compound that initial tests show is highly effective at preventing cancer. Several years pass, and the drug comes into universal usage as a cancer preventative…</p>
    <p>…until one day, years down the line, a molecular clock embedded in the compound causes it to produce a potent toxin that suddenly kills anyone with trace amounts of the substance in their bodies.</p>
    <p>It turns out the algorithm had found that the compound that was most effective at driving cancer rates to 0 was one that killed humans before they could grow old enough to develop cancer. The system also predicted that its drug would only achieve this goal if it were widely used, so it combined the toxin with a helpful drug that would incentivize the drug’s widespread adoption.</p>
</blockquote>

  <p>Of course, the concern isn’t about this example specifically, but rather similar unintended consequences. These reemerge for almost any goal researchers have yet come up with to offer a superintelligent machine.10 And all it takes is for a <i>single</i> super-intelligent machine in the world to receive a poor instruction, and it could pose a large risk.</p>

  <p>The smarter a system, the harder it becomes for humans to exercise meaningful oversight. And, as in the scenario above, an intelligent machine will often want to keep humans in the dark, if obscuring its actions reduces the risk that humans will interfere with it achieving its assigned goal.</p>

  <p>You might think ‘why can’t we just turn it off?’, but of course an intelligent system will give every indication of doing exactly what we want, until it is certain we won’t be able to turn it off.</p>
  <p>An intelligent machine may ‘know’ that what it is doing is not what humans intended it to do, but that is simply not relevant. Just as a heat-seeking missile follows hot objects, by design a machine intelligence will do exactly, and literally, what we initially program it to do. Unfortunately, intelligence doesn’t necessarily mean it shares our goals. As a result it can easily become monomaniacal in pursuit of a supremely stupid goal.</p>
  <p>The solution is to figure out how to ensure that the instructions we give to a machine intelligence really capture what we want it to do, without any such unintended outcomes. This is called a solution to the ‘control’ or ‘value alignment’ problem.</p>
  <p>It’s hard to imagine a more important research question. Solving the control problem could mean the difference between enormous wealth, happiness and health — and the destruction of the very conditions which allow humanity to thrive.</p>

  <h3>Few people are working on the problem</h3>

  <p>While the stakes seem huge, the effort being put into avoiding these hazards is small. Global spending on research and action to ensure that machine intelligence is developed safely will come to only $9 million in 2017. By comparison, over 100 times as much is spent trying to speed up the development of machine intelligence, and 26,000 times as much is spent on biomedical research.</p>
  <p>That said, the field of AI safety research is growing quickly – in 2015, total spending was just $3 million.</p>

  <center><img src="img/section2-2.png"></center>


  <p><i>Technical research refers to work in mathematics and AI to solve the control problem. Strategy research is focused on broader questions about how to safely develop AI, such as how it should be regulated.</i></p>
  <p>Since so few resources have gone into this work so far, we can expect to make quite rapid progress early on: the easiest and most useful discoveries are still available for you to make. And the problem has become more urgent in the last few years due to <a href="https://80000hours.org/2016/01/is-now-the-time-to-do-something-about-ai/" target="_blank">increased investment and progress in developing machine intelligence</a>.</p>

  <h3>There are clear ways to make progress</h3>
  <p>As we’d expect from the above, recent investment into technical research on the control problem has already yielded significant results. We’ve detailed some of these findings in this footnote. While few of the technical issues have been resolved, we have a much clearer picture today of how intelligent systems can go wrong than a few years ago, which is the first step towards a solution.</p>
  <p>There has also been recent progress on better understanding the broader ‘strategic’ issues around AI. For instance, there has been research into how the government should respond to AI, covering arms races, the implications of sharing research openly, and the criteria on which AI policy should be judged. That said, there is still very little written on these topics, so single papers can be a huge contribution to the literature.</p>
  <p>Even if – as some have argued – meaningful research were not possible right now, it would still be possible to <a href="https://80000hours.org/2015/12/even-if-we-cant-lower-catastrophic-risks-now-we-should-do-something-now-so-we-can-do-more-later/" target="_blank">build a community</a> dedicated to mitigating these risks at a future time when progress is easier. Work by non-technical people has helped to expand funding and interest in the field a great deal, contributing to the recent rapid growth in efforts to tackle the problem.</p>


</div>
<div id="section4" class="container-fluid">
  <h1>What are the major arguments against this problem being pressing?</h1>
  <h3>Not everyone agrees that there is a problem</h3>
  <p>Here are some examples of counterarguments:</p>
  <ul>
    <li>Some argue that machine intelligences beyond human capabilities are a long way away. For examples of this debate see <a href="https://www.technologyreview.com/s/602410/no-the-experts-dont-think-superintelligent-ai-is-a-threat-to-humanity/" target="_blank">No, the Experts Don’t Think Superintelligent AI is a Threat to Humanity</a> in the MIT Technology Review, and the responses <a href="https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/" target="_blank">Yes, We Are Worried About the Existential Risk of Artificial Intelligence</a>, and the <a href="http://web.archive.org/web/20170125083304/http://slatestarcodex.com/ai-persuasion-experiment-essay-b/" target="_blank">AI FAQ</a>.</li>
    <li>Some believe that artificial intelligence, even if much more intelligent than humans in some ways, will never have the opportunity to cause destruction on a global scale. For an example of this, see economist Robin Hanson, who believes that machines will eventually become better than humans at all tasks and supercede us, but that the <a href="http://www.overcomingbias.com/2014/07/30855.html" target="_blank">process will be gradual and distributed enough</a> to ensure that no one actor is ever in a position to become particularly influential. His views are detailed in his book The Age of Em.</li>
    <li>Some believe that it will be straightforward to get an intelligent system to act in our interests. For an example of this, see Holden Karnofsky <a href="http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/" target="_blank">arguing in 2012</a> that we could design AIs to work as passive tools rather than active agents (though he has since <a href="http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity" target="_blank">changed his view</a> significantly and now represents one of the field’s major funders).</li>
    <li>Neil Lawrence, an academic in machine learning, <a href="http://inverseprobability.com/2016/05/09/machine-learning-futures-6" target="_blank">takes issue with</a> many predictions in Bostrom’s book Superintelligence, including our ability to make meaningful predictions far into the future.</li>
  </ul>
  <p>The fact that there isn’t a consensus that smarter than human AI is coming soon and will be dangerous is a relief. However, given that a significant and growing fraction of relevant experts are worried, it’s a matter of prudent risk management to put effort into the problem in case they are right. You don’t need to be 100% sure your house is going to burn down to buy fire insurance.</p>
  <p>We aren’t the most qualified to judge, but we have looked into the substantive issues and mostly found ourselves agreeing with those who are more worried than less.</p>

  <h3>It may be too early to work on it</h3>

  <p>If the development of human-level machine intelligence is hundreds of years away, then it may be premature to research how to align it with human values. For example, the methods used to build machine intelligence may end up being completely different from those we use to develop AI now, rendering today’s research obsolete.</p>
  <p>However, the surveys of computer scientists show that there’s a significant chance – perhaps around 10% – that human level AI will arrive in 10-20 years. It’s worth starting now just in case this fast scenario proves to be accurate.</p>
  <p>Furthermore, even if we knew that human level AI was at least 50 years away, we don’t know how hard it will be to solve the ‘control problem’. The solution may require a series of insights that naturally come one after another. The more of those insights we build up ahead of time, the more likely it is that we’ll be able to complete the solution in a rush once the nature of AI becomes clear.</p>
  <p>Additionally, acting today could set up the infrastructure necessary to take action later, even if research today is not directly helpful.</p>

  <h3>It could be very hard to solve</h3>

  <p>As with many research projects in their early stages, we don’t know how hard this problem is to solve. Someone could believe there are major risks from machine intelligence, but be pessimistic about what additional research will accomplish, and so decide not to focus on it.</p>

  <h3>It may not fit your skills</h3>
  <p>Many individuals are concerned about this problem, but think that their skills are not a natural fit for working on it, so spend their time working on something else. This is likely true for math-heavy technical research roles, though below we also describe operational and support roles that are a good fit for a wider range of people.</p>

  <h3>Summing up</h3>
  <ul>
    <li>It is probably possible to design a machine that is as good at accomplishing its goals as humans, including ‘social’ tasks that machines are currently hopeless at. Experts in artificial intelligence assign a greater than 50% chance of this happening in the 21st century.</li>
    <li>Without careful design for reliability and robustness, machine intelligence may do things very differently than what humans intended – including pursuing policies that have a catastrophic impact on the world.</li>
    <li>Even if advanced machine intelligence does not get ‘out of control’, it is likely to be very socially disruptive and could be used as a destabilizing weapon of war.</li>
    <li>It is unknown how fast progress on this problem can be made – it may be fast, or slow.</li>
  </ul>


</div>
<div id="section5" class="container-fluid">
  <h1>What can you do to help?</h1>
  <p>We’ve broken this section into five parts to cover the main paths to making a difference in this area.</p>
  <h3>1. Technical research</h3>
  <p>Ultimately the problem will require a <i>technical</i> solution – humans will need to find a way to ensure that machines always understand and comply with what we really want them to do. But few people are able to do this research, and there’s currently a surplus of funding and a shortage of researchers.</p>
  <p>So, if you might be a good fit for this kind of research, it could well be one of the highest-impact things you can do with your life.</p>
  <p>Researchers in this field mostly work in academia and technology companies such as <a href="https://deepmind.com/" target="_blank">Google Deepmind</a> or <a href="https://openai.com/" target="_blank">OpenAI</a>. You might be a good fit if you would be capable of completing a PhD at a top 20 program in computer science or a similar quantitative course (though it’s not necessary to have such a background). We discuss this path in detail <a href="https://80000hours.org/career-reviews/artificial-intelligence-risk-research/" target="_blank">here</a>:</p>

  <h3>2. Strategy and policy research</h3>
  <p>If improvements in artificial intelligence come to represent the most important changes in the 21st century, governments are sure to take a keen interest. For this reason, there is a lot of interest in strategic and policy research – attempts to forecast how a transition to smarter-than-human machine intelligence could occur, and what the response by governments and other major institutions should be.</p>
  <p>This is a huge field, but some key issues include:</p>
  <ol>
    <li>How should we respond to technological unemployment if intelligent systems rapidly displace human workers?</li>
    <li>How do we avoid an ‘arms race’ in which countries or organizations race to develop strong machine intelligences, for strategic advantage, as occurred with nuclear weapons?</li>
    <li>When, if ever, should we expect AI to achieve particular capabilities or reach human-level intelligence?</li>
  </ol>
  <p>If we handle these issues badly, it could lead to disaster, even if we can solve the technical challenges associated with controlling a machine intelligence. So there’s a real need for more people to work on them</p>

  <h3>3. Complementary roles</h3>
  <p>Even in a research organization, around half of the staff will be doing other tasks essential for the organization to continue functioning and have an impact. Having high-performing people in these roles is essential. Better staff allow an organization to grow more quickly, avoid major mistakes, and have a larger impact by communicating its ideas to more people.</p>
  <p>Our impression is that the importance of these roles is underrated because the work is less visible. Some of the people who have made the largest contributions to solving this problem have done so as communicators and project managers. In addition, these roles are a good fit for a large number of people.</p>
  <p>Organizations working on AI safety need a wide range of complementary skills:</p>
  <ul>
    <li>HR;</li>
    <li>personnel management;</li>
    <li>project management;</li>
    <li>event management;</li>
    <li>media and communications;</li>
    <li>visual and <a href="https://80000hours.org/career-reviews/web-designer/" target="_blank">website design</a>;</li>
    <li>accounting.</li>
  </ul>
  <p>This path is open to many people who can perform at a high level in these skills.</p>
  <p>To get into these roles you’ll want to get similar jobs in organizations known for requiring high-quality work and investing in training their staff. We have more about how to skill up in our article on <a href="https://80000hours.org/career-guide/career-capital/#which-jobs-are-best-early-career" target="_blank">career capital</a>.</p>

  <h3>4. Advocacy and capacity building</h3>
  <p>People who are relatively strong on social skills might be able to have a larger impact by persuading others to work on or fund the problem. This is usually done by working at one of the research organizations already mentioned.</p>
  <p>Beyond that, the group we know that is doing this the most to raise awareness of the issue is the <a href="http://www.effectivealtruism.org/" target="_blank">effective altruism community</a>, of which we are a part. Joining and growing that movement is a promising way to increase efforts to solve the AI control problem, among other pressing problems.</p>
  <p>Once you are familiar with the issue, you could also spend some of your time spreading the word in any of the careers that typically provide you with a platform for advocacy, such as:</p>
  <ul>
    <li><a href="https://80000hours.org/career-reviews/journalism/" target="_blank">Journalism and writing</a></li>
    <li><a href="https://80000hours.org/career-reviews/party-politics-uk/" target="_blank">Party politics</a></li>
    <li><a href="https://80000hours.org/career-reviews/policy-oriented-civil-service-uk/" target="_blank">Policy-oriented civil service</a></li>
    <li><a href="https://80000hours.org/career-reviews/valuable-academic-research/" target="_blank">Academia.</a></li>
  </ul>
  <p>You could also rise up the ranks of an organization doing some relevant work, such as Google or the US military, and promote concern for AI safety there.</p>
  <p>However, unless you are doing the technical, strategic or policy research described above, you will probably only be able to spend a fraction of your time on this work.</p>
  <p>We would also caution that it is easy to do harm while engaging in advocacy about AI. If portrayed in a sensationalist manner, or by someone without necessary technical understanding, ‘advocacy’ can in fact simply be confusing and make the issue appear less credible. Much coverage of this topic in the media misrepresents the concerns actually held by experts. To avoid contributing to this we strongly recommend informing yourself thoroughly and presenting any information in a sober, accurate manner.</p>

  <h3>5. Earning to give</h3>
  <p>There is an increasing amount of funding available for research in this area, and we expect more large funders to enter the field in future. That means the problem is primarily <a href="https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/" target="_blank">talent constrained</a> – especially by a need for innovative researchers.</p>
  <p>However, there are still some funding gaps, especially among the less conventional groups that can’t get academic grants, such as the Machine Intelligence Research Institute.</p>
  <p>As a result <a href="https://80000hours.org/career-guide/high-impact-jobs/#approach-1-earning-to-give" target="_blank">earning to give</a> to support others working on the problem directly is still a reasonable option if you don’t feel the other roles described here are a good fit for you.</p>
  <p>If you <a href="https://app.effectivealtruism.org/funds/far-future" target="_blank">want to donate</a>, our first suggestion is giving to the Long Term Future Fund. The manager of the fund is an expert in catastrophic risk funding, and makes grants to the organizations that are most in need of funding at the time. It’s run by the Centre for Effective Altruism, of which we’re part.</p>
  <p>Alternatively you can choose for yourself among the top non-profit organizations in the area, such as the Machine Intelligence Research Institute in Berkeley and the Future of Humanity Institute at Oxford. These were the most popular options among experts in <a href="https://80000hours.org/2016/12/the-effective-altruism-guide-to-donating-this-giving-season/#ai" target="_blank">our review in December 2016</a>. See more organizations below.</p>

  <h3>What are the key organizations you could work for?</h3>
  <p>We keep a list of every organization that we know is working on AI safety, with links to their vacancies pages, <a href="http://wiki.80000hours.org/index.php/Places_we_sometimes_recommend_people_apply_to_work#Risks_from_artificial_intelligence" target="_blank">here</a>. The ten most significant organizations, all of which would be good places to work, are probably the following:</p>

  <ul>
    <li><a href="https://en.wikipedia.org/wiki/DeepMind" target="_blank"><b>Google DeepMind</b></a> is probably the largest and most advanced research group developing general machine intelligence. It includes a number of staff working on safety and ethics issues specifically. <a href="https://deepmind.com/careers/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>. <a href="https://en.wikipedia.org/wiki/Google_Brain" target="_blank"><b>Google Brain</b></a> is another deep learning research project at Google. <a href="https://research.google.com/teams/brain/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li>The <a href="https://www.fhi.ox.ac.uk/" target="_blank"><b>Future of Humanity Institute</b></a> at Oxford University was founded by Prof Nick Bostrom, author of <a href="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834" target="_blank">Superintelligence</a>. It has a number of academic staff conducting both technical and strategic research. <a href="https://www.fhi.ox.ac.uk/vacancies/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li><a href="https://openai.com/" target="_blank"><b>OpenAI</b></a> was founded in 2015 with the goal of conducting research into how to make AI safe and freely sharing the information. It has received $1 billion in funding commitments from the technology community. <a href="https://jobs.lever.co/openai" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li>The <a href="http://intelligence.org/" target="_blank"><b>Machine Intelligence Research Institute</b></a> (MIRI) was one of the first groups to become concerned about the risks from machine intelligence in the early 2000s, and has published a <a href="https://intelligence.org/research/" target="_blank">number of papers</a> on safety issues and how to resolve them. <a href="https://intelligence.org/get-involved/#careers" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li>The <a href="http://cser.org/" target="_blank"><b>Cambridge Centre for the Study of Existential Risk</b></a> and <a href="http://lcfi.ac.uk/" target="_blank"><b>Leverhulme Centre for the Study for the Future of Intelligence</b></a> at Cambridge University house academics studying both technical and strategic questions related to AI safety. <a href="http://cser.org/vacancies/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li>The <a href="http://humancompatible.ai/jobs/" target="_blank"><b>Berkeley Center for Human-Compatible Artificial Intelligence</b></a> is very new, but intends to conduct primarily technical research, with a budget of several million dollars a year. <a href="http://humancompatible.ai/jobs/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li>The <a href="https://futureoflife.org/" target="_blank">Future of Life Institute</a> at MIT does a combination of communications and grant-making to organizations in the AI safety space, in addition to work on the risks from nuclear war and pandemics. <a href="https://futureoflife.org/job-postings/" target="_blank">See current vacancies and subscribe to get notified of new job openings</a>.</li>
    <li><b>Alan Dafoe’s research group at Yale University</b> is <a href="http://www.allandafoe.com/aiclass" target="_blank">conducting research</a> on the ‘global politics of AI’, including its effects on international conflict. PhD or research assistant positions may be available.</li>
    <li><a href="http://aiimpacts.org/" target="_blank"><b>AI Impacts</b></a> is a non-profit which works on forecasting progress in machine intelligence and predicting its likely impacts.</li>
  </ul>


</div>
<div id="section6" class="container-fluid">
  <h1>List of all our articles on artificial intelligence</h1>
  <p>We offer a number of other resources about AI-related careers. Here they are, in the order that you might naturally read them:</p>
  <ol>
    <li><a href="https://80000hours.org/career-guide/world-problems/" target="_blank">Introduction to how we try to prioritise the world’s problems, and where AI fits into the bigger picture</a></li>
    <li><a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/" target="_blank">This problem profile on positively shaping artificial intelligence</a> – why we think this problem is pressing, and what can be done about it</li>
    <li><a href="https://80000hours.org/career-reviews/artificial-intelligence-risk-research/" target="_blank">Guide to pursuing a career in technical AI safety research</a>, and a <a href="https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/" target="_blank">related recorded interview with a researcher at OpenAI</a>.</li>
    <li><a href="https://80000hours.org/articles/ai-policy-guide/" target="_blank">Guide to pursuing a career in AI strategy and policy</a>, along with <a href="https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one/" target="_blank">an interview with the author of this guide</a></li>
    <li><a href="https://80000hours.org/ai-safety-syllabus/" target="_blank">AI Safety Syllabus</a> – a compilation of useful things to read to learn more, as well as places to study and conferences to attend.</li>
  <p>Also relevant for some will be our career reviews of <a href="https://80000hours.org/career-reviews/machine-learning-phd/" target="_blank">Machine Learning PhDs</a> and <a href="https://80000hours.org/career-reviews/data-science/" target="_blank">Data Science</a>.</p>

</div>

<div id="section7" class="container-fluid">
  <h1>Learn more</h1>
  <p>The top two introductory sources are:</p>

  <ul>
    <li>Prof Bostrom’s TED talk <a href="https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are" target="_blank">outlining the problem and what can be done about it</a>.</li>
    <li><a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html" target="_blank"><i>The Artificial Intelligence Revolution</i></a>, by Tim Urban at Wait But Why. (And also see <a href="http://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/" target="_blank">this response)</a>.</li>
  </ul>

  <p>After that:</p>

  <ul>
    <li>The key work describing the problem in detail is <a href="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/01987398340" target="_blank">Superintelligence</a>. If you’re not ready for a full book try <a href="https://aisafety.wordpress.com/" target="_blank">this somewhat more technical article by Michael Cohen</a>.</li>
    <li>If you want to do technical research our AI safety syllabus lists a range of sources for <a href="https://80000hours.org/ai-safety-syllabus/" target="_blank">technical information on the problem</a>. It’s ideal to work through these articles over a period of time.</li>
  </ul>


</div>
<div id="section8" class="container-fluid">
  <h1>Send us a suggestion</h1>
  <form action="index.php" method="post">
    <div class="form-group">
      <label for="name">Name:</label>
      <input type="name" class="form-control" id="name" placeholder="Enter name" name="name">
    </div>
    <div class="form-group">
      <label for="email">Email Adress:</label>
      <input type="email" class="form-control" id="endelet" placeholder="Enter email" name="endelet">
    </div>
    <div class="form-group">
      <label for="suggestion">Suggestion:</label>
      <textarea class="form-control" rows="5" id="suggestion" placeholder="Your suggestion here" name="suggestion"></textarea>
    </div>
    <button type="submit" class="btn btn-default">Submit</button>
  </form>

  <p style="text-align: Center;">Desafio Programação | Developed by Thaís Carvalho | 2018 | <a href="consulta.php" target="_blank">Report</a></p>
</div>

</body>
</html>
